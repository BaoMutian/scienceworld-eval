# ScienceWorld Evaluation Configuration

# LLM Service Configuration
llm:
  api_base_url: "https://openrouter.ai/api/v1"
  api_key: "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" # Set via OPENROUTER_API_KEY or OPENAI_API_KEY env var
  model: "qwen/qwen-2.5-72b-instruct"
  temperature: 0.3
  max_tokens: 1024
  timeout: 60

  # Model-specific extra parameters (passed directly to OpenAI-compatible API)
  # These are standard API parameters like top_p, presence_penalty, etc.
  #
  # Note for Qwen3 (https://huggingface.co/Qwen/Qwen3-8B):
  #   - enable_thinking is a vLLM SERVER startup param, NOT an API param
  #   - Start vLLM with: vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1
  #   - Or use /think and /no_think in user messages to toggle thinking mode
  #   - Thinking mode: temperature=0.6, top_p=0.95 (avoid greedy decoding!)
  #   - Non-thinking mode: temperature=0.7, top_p=0.8
  #
  extra_params: {}
  # extra_params:
  #   top_p: 0.95

# Retry Configuration
retry:
  max_retries: 3
  retry_interval: 1.0
  max_retry_interval: 8.0

# Test Configuration
test:
  num_episodes: 3 # Number of variations to test per task (1 episode each)
  task_ids: ["1-1"] # null = all tasks, or list like ["1-1", "4-1", "4-2"]
  split: "dev" # train/dev/test
  seed: 42
  max_steps: 30 # Maximum steps per episode
  simplifications: "easy" # easy, or comma-separated: "teleportAction,openDoors"

# Prompt Configuration
prompt:
  use_few_shot: true
  history_length: 30 # Number of recent history entries to include

# Runtime Configuration
runtime:
  save_interval: 1 # Save checkpoint every N episodes
  output_dir: "results"
  debug: true

# Memory Configuration (ReasoningBank)
memory:
  enabled: true
  mode: "retrieve_and_extract" # baseline | retrieve_only | retrieve_and_extract

  # Storage configuration
  memory_dir: "memory_banks"
  task_name: "scienceworld"

  # Embedding model configuration
  embedding_model: "BAAI/bge-base-en-v1.5"
  embedding_device: "cpu"

  # Retrieval parameters
  top_k: 1
  similarity_threshold: 0.5

  # MaTTS configuration (Memory-aware Test-Time Scaling)
  matts:
    enabled: false
    sample_n: 3
    temperature: 0.7
    max_tokens: 1024
